{"title":"机器学习系列笔记（一）","uid":"127b640a2365c521fe651e857a784559","slug":"20210829","date":"2021-08-29T06:56:50.000Z","updated":"2021-08-29T10:18:53.784Z","comments":true,"path":"api/articles/20210829.json","keywords":null,"cover":[],"content":"<p>最近在训练一个5000个类别的分类模型，每次调参后训练时间都很长，打算听从我导的建议，利用这些碎片时间把深度学习系统地过一遍。主要参考<a href=\"https://www.bilibili.com/video/BV1Wv411h7kN\">李宏毅</a>和《深度学习》。在这里做一个学习记录便于后续复习参考。</p>\n<p>有小伙伴提到评论区无法正常使用的问题，等我后续有空再进行调整，在此之前如果大家对博客中的内容有建议/疑问欢迎通过<a href=\"https://github.com/Roxanne718/Roxanne718.github.io/issues\">issue</a>交流。</p>\n<p>感谢阅读！</p>\n<span id=\"more\"></span>\n\n<h2 id=\"ML-amp-DL\"><a href=\"#ML-amp-DL\" class=\"headerlink\" title=\"ML &amp; DL\"></a>ML &amp; DL</h2><ul>\n<li>ML：定义 function -&gt; 对参数（权重）优化 -&gt; 选择最佳权重</li>\n<li>DL：定义神经网络 -&gt; 对参数（权重）优化 -&gt; 选择最佳权重</li>\n</ul>\n<p>我们可以把一个 Logistic Regression 理解为一个<strong>神经元</strong>，用不同方法连接这些神经元，就得到了不同的神经网络。</p>\n<p><img src=\"/post/20210829/LR.jpg\" alt=\"LR公式\"></p>\n<p>比如，分成多列、完全连接，就得到了<strong>全连接神经网络</strong>。</p>\n<p><img src=\"/post/20210829/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg\" alt=\"全连接神经网络\"></p>\n<p>如上图，上图是一个确定的全连接神经网络，我们可以把它看作是一个 function ，它的输入是一个向量，输出也是一个向量。</p>\n<p><img src=\"/post/20210829/function.jpg\" alt=\"function\"></p>\n<p>如果改变这个 function 里的神经元 weights 和 bias，就会得到另一个 function。换言之，这个结构定义了一个<strong>函数集合</strong>。DL的训练过程其实就是通过梯度下降等方法找到让 function 表现最好的W和b们。</p>\n<p>在做具体预测的时候，我们会用<strong>矩阵操作</strong>进行计算，每一个 weights 用一个矩阵表示，每一个 bias 用一个向量表示，那么一个 Logistic regression 就是 $x’ = sigmoid(Wx+b)$ ,再把$x’$传递给下一个LR。</p>\n<p>$$Output = Sigmoid( Sigmoid(…Sigmoid(Wx+b)+b)+b )$$</p>\n<p><em>DL和 ML 相比，最大的不同是把 ML 中需要人工参与的<strong>特征提取</strong>放到神经网络中去，让神经网络来寻找能表示训练数据的特征向量。DL 可以解决人类不能解释的问题（如语音识别等）， ML 适合做人类能比较好地描述特征的任务。</em></p>\n<h2 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h2><p>在通过梯度下降优化 $W$ 和 $b$ 时，神经网络和 Linear Regression 没有区别，都是先求偏导，更新 W 和 b。难点在于神经网络的函数更加复杂，需要通过<strong>链式求导</strong>来计算梯度。</p>\n<p>下面介绍一个简单的例子。</p>\n<p><img src=\"/post/20210829/BP-1.jpg\" alt=\"BP-1\"></p>\n<p>将某一个训练数据$x^n$传入神经网络，得到预测的$y^n$,定义这个预测值$y^n$和真实值$\\hat{y}^n$的差距为$C^n$，那么整个数据集的损失函数为：<br>$$ L(θ) = \\sum_{n=1}^NC^n(θ)$$</p>\n<p>对于其中某一个$W$, 有：</p>\n<p><img src=\"/post/20210829/BP-2.jpg\" alt=\"BP-2\"></p>\n<p>只要计算某一个训练数据对$W$的偏导，再对所有训练数据进行sum，就能得到整个数据集对该$W$的偏导，下面计算单个数据的偏导。</p>\n<p>对于每一个神经元：</p>\n<p><img src=\"/post/20210829/BP-3.jpg\" alt=\"BP-3\"></p>\n<p>根据链式法则，C对于这个$W$的偏导是：</p>\n<p><img src=\"/post/20210829/BP-4.jpg\" alt=\"BP-4\"></p>\n<p>其中，$\\frac{\\partial z}{\\partial w}$ 比较好计算，即对应的 $x$ 值（Forward pass）。</p>\n<p><img src=\"/post/20210829/BP-5.jpg\" alt=\"BP-5\"></p>\n<p>$\\frac{\\partial C}{\\partial z}$ 比较复杂（Backward pass）。假设通过激活函数后的值为$a$, 则这个值又可以写为：</p>\n<p>$$\\frac{\\partial C}{\\partial z} = \\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}$$</p>\n<p>其中，$\\frac{\\partial a}{\\partial z}$是对激活函数求导。</p>\n<p><img src=\"/post/20210829/BP-6.jpg\" alt=\"BP-6\"></p>\n<p>$\\frac{\\partial C}{\\partial a}$和之前的<br><img src=\"/post/20210829/BP-4.jpg\" alt=\"BP-4\"><br>计算异曲同工，</p>\n<p>$$\\frac{\\partial C}{\\partial a}=\\frac{\\partial z’}{\\partial a}\\frac{\\partial C}{\\partial z’} + \\frac{\\partial z’’}{\\partial a}\\frac{\\partial C}{\\partial z’’}$$</p>\n<p>其中，$\\frac{\\partial z’}{\\partial a}$和$\\frac{\\partial z’’}{\\partial a}$的计算同样是对应的 $x$ 值。则</p>\n<p>$$\\frac{\\partial C}{\\partial z} = \\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}=sigmoid’(z)[w_3\\frac{\\partial C}{\\partial z’}+w_4\\frac{\\partial C}{\\partial z’’}]$$</p>\n<p>接下来分两种情况讨论，</p>\n<ul>\n<li>如果$z’$和$z’’$是最后一层，则计算非常简单：</li>\n</ul>\n<p><img src=\"/post/20210829/BP-7.jpg\" alt=\"BP-7\"></p>\n<ul>\n<li>如果不是最后一层，需要逐层从后往前回溯</li>\n</ul>\n<h3 id=\"总结：\"><a href=\"#总结：\" class=\"headerlink\" title=\"总结：\"></a>总结：</h3><p>BP算法：先正向传播，计算$\\frac{\\partial z}{\\partial w}=x$, 再反向传播，计算$\\frac{\\partial C}{\\partial z}$，则：<br>$$\\frac{\\partial C}{\\partial w}=\\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}$$</p>\n","feature":true,"text":"最近在训练一个5000个类别的分类模型，每次调参后训练时间都很长，打算听从我导的建议，利用这些碎片时间把深度学习系统地过一遍。主要参考李宏毅和《深度学习》。在这里做一个学习记录便于后续复习参考。 有小伙伴提到评论区无法正常使用的问题，等我后续有空再进行调整，在此之前如果大家对博客...","link":"","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[{"name":"Note","slug":"Note","count":1,"path":"api/categories/Note.json"}],"tags":[{"name":"ML","slug":"ML","count":1,"path":"api/tags/ML.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#ML-amp-DL\"><span class=\"toc-text\">ML &amp; DL</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD\"><span class=\"toc-text\">反向传播</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%80%BB%E7%BB%93%EF%BC%9A\"><span class=\"toc-text\">总结：</span></a></li></ol></li></ol>","author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"神经架构搜索简介","uid":"c570dc3eddcfb883ac8ac92a0eb2658a","slug":"20210826","date":"2021-08-25T08:48:50.000Z","updated":"2021-08-26T13:44:52.733Z","comments":true,"path":"api/articles/20210826.json","keywords":null,"cover":[],"text":"今天水一天，放一章之前入门 NAS 时做的整理~ NAS的三个研究方向 搜索空间 定义网络的表达形式 结合特定任务的先验知识，可以减小搜索空间的大小，但同时也引入了人类的偏见，可能会限制人类找到更好的未知结构 搜索策略 构建模型 一方面要提高收敛速度，另一方面应该避免过早收敛到次...","link":"","photos":[],"count_time":{"symbolsCount":638,"symbolsTime":"1 mins."},"categories":[{"name":"Review","slug":"Review","count":1,"path":"api/categories/Review.json"}],"tags":[{"name":"NAS","slug":"NAS","count":1,"path":"api/tags/NAS.json"}],"author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}