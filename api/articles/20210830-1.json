{"title":"机器学习常用公式、概念","uid":"e2d1347c7877fd7ee4f5b998819e7fec","slug":"20210830-1","date":"2021-08-30T02:45:00.000Z","updated":"2021-08-31T11:28:34.402Z","comments":true,"path":"api/articles/20210830-1.json","keywords":null,"cover":null,"content":"<p>本文对比总结在机器学习和深度学习中常见的概念和公式，以备查阅。</p>\n<p>持续更新。</p>\n<span id=\"more\"></span>\n<h2 id=\"LOSS\"><a href=\"#LOSS\" class=\"headerlink\" title=\"LOSS\"></a>LOSS</h2><p><strong>MAE</strong>: Mean Absolute Error, 差的绝对值</p>\n<p><strong>MAE</strong>：Mean Square Error, 差的平方</p>\n<p><strong>Cross-entropy</strong>： 交叉熵</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><h3 id=\"ReLU\"><a href=\"#ReLU\" class=\"headerlink\" title=\"ReLU\"></a>ReLU</h3>","text":"本文对比总结在机器学习和深度学习中常见的概念和公式，以备查阅。 持续更新。 LOSSMAE: Mean Absolute Error, 差的绝对值 MAE：Mean Square Error, 差的平方 Cross-entropy： 交叉熵 激活函数SigmoidReLU","link":"","photos":[],"count_time":{"symbolsCount":137,"symbolsTime":"1 mins."},"categories":[{"name":"Note","slug":"Note","count":8,"path":"api/categories/Note.json"}],"tags":[{"name":"ML","slug":"ML","count":3,"path":"api/tags/ML.json"},{"name":"DL","slug":"DL","count":4,"path":"api/tags/DL.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#LOSS\"><span class=\"toc-text\">LOSS</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">激活函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Sigmoid\"><span class=\"toc-text\">Sigmoid</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ReLU\"><span class=\"toc-text\">ReLU</span></a></li></ol></li></ol>","author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"机器学习系列笔记（二）","uid":"4c9f78b6f353deae14a5acac20919d3f","slug":"20210831","date":"2021-08-31T09:41:00.000Z","updated":"2021-08-31T11:29:27.299Z","comments":true,"path":"api/articles/20210831.json","keywords":null,"cover":[],"text":"这一部分讨论 Optimization 如果在梯度下降中，当 loss 已经稳定，但还没有达到期望值时，或者 loss 根本不下降，应该怎么做？ 原因猜想：loss 对参数的梯度变成了0（Critical point） 鞍点（Saddle point） 局部极值 鞍点如果是鞍点，...","link":"","photos":[],"count_time":{"symbolsCount":595,"symbolsTime":"1 mins."},"categories":[{"name":"Note","slug":"Note","count":8,"path":"api/categories/Note.json"}],"tags":[{"name":"ML","slug":"ML","count":3,"path":"api/tags/ML.json"},{"name":"DL","slug":"DL","count":4,"path":"api/tags/DL.json"}],"author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"深度学习应该有多深","uid":"9fe013286807d555c32ac577b233a122","slug":"20210830-3","date":"2021-08-30T02:45:00.000Z","updated":"2021-08-30T14:33:52.738Z","comments":true,"path":"api/articles/20210830-3.json","keywords":null,"cover":null,"text":"深度和宽度是一样的吗？深度越深越好吗？","link":"","photos":[],"count_time":{"symbolsCount":19,"symbolsTime":"1 mins."},"categories":[{"name":"Note","slug":"Note","count":8,"path":"api/categories/Note.json"}],"tags":[{"name":"DL","slug":"DL","count":4,"path":"api/tags/DL.json"}],"author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}