{"title":"深度学习应该有多深","uid":"9fe013286807d555c32ac577b233a122","slug":"20210830-3","date":"2021-08-30T02:45:00.000Z","updated":"2021-08-30T14:33:52.738Z","comments":true,"path":"api/articles/20210830-3.json","keywords":null,"cover":null,"content":"<h2 id=\"深度和宽度是一样的吗？\"><a href=\"#深度和宽度是一样的吗？\" class=\"headerlink\" title=\"深度和宽度是一样的吗？\"></a>深度和宽度是一样的吗？</h2><h2 id=\"深度越深越好吗？\"><a href=\"#深度越深越好吗？\" class=\"headerlink\" title=\"深度越深越好吗？\"></a>深度越深越好吗？</h2>","text":"深度和宽度是一样的吗？深度越深越好吗？","link":"","photos":[],"count_time":{"symbolsCount":19,"symbolsTime":"1 mins."},"categories":[{"name":"Note","slug":"Note","count":8,"path":"api/categories/Note.json"}],"tags":[{"name":"DL","slug":"DL","count":4,"path":"api/tags/DL.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E5%92%8C%E5%AE%BD%E5%BA%A6%E6%98%AF%E4%B8%80%E6%A0%B7%E7%9A%84%E5%90%97%EF%BC%9F\"><span class=\"toc-text\">深度和宽度是一样的吗？</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD%E5%90%97%EF%BC%9F\"><span class=\"toc-text\">深度越深越好吗？</span></a></li></ol>","author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"机器学习常用公式、概念","uid":"e2d1347c7877fd7ee4f5b998819e7fec","slug":"20210830-1","date":"2021-08-30T02:45:00.000Z","updated":"2021-08-31T11:28:34.402Z","comments":true,"path":"api/articles/20210830-1.json","keywords":null,"cover":null,"text":"本文对比总结在机器学习和深度学习中常见的概念和公式，以备查阅。 持续更新。 LOSSMAE: Mean Absolute Error, 差的绝对值 MAE：Mean Square Error, 差的平方 Cross-entropy： 交叉熵 激活函数SigmoidReLU","link":"","photos":[],"count_time":{"symbolsCount":137,"symbolsTime":"1 mins."},"categories":[{"name":"Note","slug":"Note","count":8,"path":"api/categories/Note.json"}],"tags":[{"name":"ML","slug":"ML","count":3,"path":"api/tags/ML.json"},{"name":"DL","slug":"DL","count":4,"path":"api/tags/DL.json"}],"author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Batch大小对训练的影响","uid":"938697651832116ebe2e74312921919c","slug":"20210830-2","date":"2021-08-30T02:45:00.000Z","updated":"2021-08-31T11:08:49.261Z","comments":true,"path":"api/articles/20210830-2.json","keywords":null,"cover":[],"text":"我们往往将数据集分成多个 batch 传入模型中，batch大小不仅影响着 GPU 的利用率，也影响着模型的训练过程。 在训练过程中，输入一个 batch ，模型会根据这个 batch 更新一次参数（一次update），也就是说，如果有100个 batch ，模型在每个epoch...","link":"","photos":[],"count_time":{"symbolsCount":403,"symbolsTime":"1 mins."},"categories":[{"name":"Note","slug":"Note","count":8,"path":"api/categories/Note.json"}],"tags":[{"name":"DL","slug":"DL","count":4,"path":"api/tags/DL.json"}],"author":{"name":"Fairy","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}