[{"id":"127b640a2365c521fe651e857a784559","title":"机器学习系列笔记（一）","content":"最近在训练一个5000个类别的分类模型，每次调参后训练时间都很长，打算听从我导的建议，利用这些碎片时间把深度学习系统地过一遍。主要参考李宏毅和《深度学习》。在这里做一个学习记录便于后续复习参考。\n有小伙伴提到评论区无法正常使用的问题，等我后续有空再进行调整，在此之前如果大家对博客中的内容有建议/疑问欢迎通过issue交流。\n感谢阅读！\n\n\nML &amp; DL\nML：定义 function -&gt; 对参数（权重）优化 -&gt; 选择最佳权重\nDL：定义神经网络 -&gt; 对参数（权重）优化 -&gt; 选择最佳权重\n\n我们可以把一个 Logistic Regression 理解为一个神经元，用不同方法连接这些神经元，就得到了不同的神经网络。\n\n比如，分成多列、完全连接，就得到了全连接神经网络。\n\n如上图，上图是一个确定的全连接神经网络，我们可以把它看作是一个 function ，它的输入是一个向量，输出也是一个向量。\n\n如果改变这个 function 里的神经元 weights 和 bias，就会得到另一个 function。换言之，这个结构定义了一个函数集合。DL的训练过程其实就是通过梯度下降等方法找到让 function 表现最好的W和b们。\n在做具体预测的时候，我们会用矩阵操作进行计算，每一个 weights 用一个矩阵表示，每一个 bias 用一个向量表示，那么一个 Logistic regression 就是 $x’ = sigmoid(Wx+b)$ ,再把$x’$传递给下一个LR。\n$$Output = Sigmoid( Sigmoid(…Sigmoid(Wx+b)+b)+b )$$\nDL和 ML 相比，最大的不同是把 ML 中需要人工参与的特征提取放到神经网络中去，让神经网络来寻找能表示训练数据的特征向量。DL 可以解决人类不能解释的问题（如语音识别等）， ML 适合做人类能比较好地描述特征的任务。\n反向传播在通过梯度下降优化 $W$ 和 $b$ 时，神经网络和 Linear Regression 没有区别，都是先求偏导，更新 W 和 b。难点在于神经网络的函数更加复杂，需要通过链式求导来计算梯度。\n下面介绍一个简单的例子。\n\n将某一个训练数据$x^n$传入神经网络，得到预测的$y^n$,定义这个预测值$y^n$和真实值$\\hat{y}^n$的差距为$C^n$，那么整个数据集的损失函数为：$$ L(θ) = \\sum_{n=1}^NC^n(θ)$$\n对于其中某一个$W$, 有：\n\n只要计算某一个训练数据对$W$的偏导，再对所有训练数据进行sum，就能得到整个数据集对该$W$的偏导，下面计算单个数据的偏导。\n对于每一个神经元：\n\n根据链式法则，C对于这个$W$的偏导是：\n\n其中，$\\frac{\\partial z}{\\partial w}$ 比较好计算，即对应的 $x$ 值（Forward pass）。\n\n$\\frac{\\partial C}{\\partial z}$ 比较复杂（Backward pass）。假设通过激活函数后的值为$a$, 则这个值又可以写为：\n$$\\frac{\\partial C}{\\partial z} = \\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}$$\n其中，$\\frac{\\partial a}{\\partial z}$是对激活函数求导。\n\n$\\frac{\\partial C}{\\partial a}$和之前的计算异曲同工，\n$$\\frac{\\partial C}{\\partial a}=\\frac{\\partial z’}{\\partial a}\\frac{\\partial C}{\\partial z’} + \\frac{\\partial z’’}{\\partial a}\\frac{\\partial C}{\\partial z’’}$$\n其中，$\\frac{\\partial z’}{\\partial a}$和$\\frac{\\partial z’’}{\\partial a}$的计算同样是对应的 $x$ 值。则\n$$\\frac{\\partial C}{\\partial z} = \\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}=sigmoid’(z)[w_3\\frac{\\partial C}{\\partial z’}+w_4\\frac{\\partial C}{\\partial z’’}]$$\n接下来分两种情况讨论，\n\n如果$z’$和$z’’$是最后一层，则计算非常简单：\n\n\n\n如果不是最后一层，需要逐层从后往前回溯\n\n总结：BP算法：先正向传播，计算$\\frac{\\partial z}{\\partial w}=x$, 再反向传播，计算$\\frac{\\partial C}{\\partial z}$，则：$$\\frac{\\partial C}{\\partial w}=\\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}$$\n","slug":"20210829","date":"2021-08-29T06:56:50.000Z","categories_index":"Note","tags_index":"ML","author_index":"Fairy"},{"id":"c570dc3eddcfb883ac8ac92a0eb2658a","title":"神经架构搜索简介","content":"今天水一天，放一章之前入门 NAS 时做的整理~\n\n\nNAS的三个研究方向\n\n搜索空间\n\n定义网络的表达形式\n结合特定任务的先验知识，可以减小搜索空间的大小，但同时也引入了人类的偏见，可能会限制人类找到更好的未知结构 \n\n\n搜索策略\n\n构建模型\n一方面要提高收敛速度，另一方面应该避免过早收敛到次优结构\n\n\n性能评价策略\n\n直接在数据集上跑模型非常耗时，需要更好的评价策略\n判断模型，反馈给搜索策略，搜索策略根据反馈重新生成模型\n\n\n\n搜索空间\n链式结构（逐层）\n\n难点：参数化\n最大层数（可能无限）\n每层的操作（池化、卷积、……）\n每层操作对应的参数\n\n\n\n\n多分支网络（逐层）\n\n每一层的输入是之前层的组合\n特例：\n链式结构\nResidual Networks（summed）\nDenseNets（concatenated）\n\n\n\n\n逐块:普通块和归约块、堆叠\n\n优势：\n搜索空间减少\n更好迁移（？）\n通过重读构建块来创建体系已经被证明的是很好的设计原则\n\n\n同时，出现了新的设计选择:\n微结构和整体结构都要最优\n微结构（可以借鉴现有的经典网络）\n\n\n\n\n\n搜索策略\n基于梯度的算法\n贝叶斯优化\n进化算法\n随机搜索（RS）\n强化学习（RL）\n遗传算法\n\n性能评估\n低保真度\n曲线外推\nNetwork Morphism  根据以前训练过的其他架构的权重来初始化新架构的权重\nOne-Shot Architecture Search  将所有体系结构视为一个超图的不同子图\n\n","slug":"20210826","date":"2021-08-25T08:48:50.000Z","categories_index":"Review","tags_index":"NAS","author_index":"Fairy"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"My first blog!\n\n本拖延癌晚期患者终于搭建了自己的博客框架 ~尽量日更~\n","slug":"20210825","date":"2021-08-25T08:48:50.000Z","categories_index":"Diary","tags_index":"Thinking","author_index":"Fairy"}]