[{"id":"4c9f78b6f353deae14a5acac20919d3f","title":"机器学习系列笔记（二）","content":"这一部分讨论 Optimization\n\n如果在梯度下降中，当 loss 已经稳定，但还没有达到期望值时，或者 loss 根本不下降，应该怎么做？\n原因猜想：loss 对参数的梯度变成了0（Critical point）\n\n鞍点（Saddle point）\n\n局部极值\n\n\n鞍点如果是鞍点，还有路可走。怎么判断是在局部极值还是鞍点？需要知道 Loss 的形状。根据泰勒级数展开，可以得到给定参数$θ’$附近的 Loss 形状：\n\ng是梯度向量，H是Hessian矩阵，即二次导数。\n在 Critical point， g=0， L的变化只需要考虑 Hessian 矩阵。\n\n\n如果H是正定矩阵（特征值全部大于0），这个点是局部最小\n如果H是负定矩阵（特征值全部小于0），这个点是局部最大\n如果H的特征值有正有负，这个点是鞍点\n\n如果是鞍点，找到负的特征值对应的特征向量，沿着这个特征向量 update 即可。\n\n但是实际情况下，H矩阵的计算非常复杂，所以通常不用这个方法。后面还会介绍更简单的方法。\n局部极值和鞍点哪一个更常见？：从更高维度看，低维度的局部极值很可能是一个鞍点，所以当维度较高时，鞍点可能更多。经验上看，局部极值也不常见。\nMomentum在一般的梯度下降的基础上：Update 的方向是:学习率*梯度负方向+另一个超参数*前一步的方向.这是一种跳出局部极值和鞍点的方法。\n","slug":"20210831","date":"2021-08-31T09:41:00.000Z","categories_index":"Note","tags_index":"ML,DL","author_index":"Fairy"},{"id":"9fe013286807d555c32ac577b233a122","title":"深度学习应该有多深","content":"深度和宽度是一样的吗？深度越深越好吗？","slug":"20210830-3","date":"2021-08-30T02:45:00.000Z","categories_index":"Note","tags_index":"DL","author_index":"Fairy"},{"id":"e2d1347c7877fd7ee4f5b998819e7fec","title":"机器学习常用公式、概念","content":"本文对比总结在机器学习和深度学习中常见的概念和公式，以备查阅。\n持续更新。\n\nLOSSMAE: Mean Absolute Error, 差的绝对值\nMAE：Mean Square Error, 差的平方\nCross-entropy： 交叉熵\n激活函数SigmoidReLU","slug":"20210830-1","date":"2021-08-30T02:45:00.000Z","categories_index":"Note","tags_index":"ML,DL","author_index":"Fairy"},{"id":"938697651832116ebe2e74312921919c","title":"Batch大小对训练的影响","content":"我们往往将数据集分成多个 batch 传入模型中，batch大小不仅影响着 GPU 的利用率，也影响着模型的训练过程。\n在训练过程中，输入一个 batch ，模型会根据这个 batch 更新一次参数（一次update），也就是说，如果有100个 batch ，模型在每个epoch就会更新100次参数。\n\n由于GPU的并行加速，在一定范围内，batch size较大时，update次数少，速度反而和 batch size 较小时差不多。\n但是 batch size 较小时，每一个 batch 的 loss 略有差异， 容易避开局部极值或者鞍点；此外，小的 batch size 更容易避开 overfitting， 这可能是因为训练集和测试集的分布可能略有差异, 而小 batch 的 update 次数更多，更倾向于走进平缓地区的局部极值。\n\n\n参考资料一些在大的batch下提高模型表现的方法。\n\n","slug":"20210830-2","date":"2021-08-30T02:45:00.000Z","categories_index":"Note","tags_index":"DL","author_index":"Fairy"},{"id":"127b640a2365c521fe651e857a784559","title":"机器学习系列笔记（一）","content":"最近在训练一个5000个类别的分类模型，每次调参后训练时间都很长，打算听从我导的建议，利用这些碎片时间把深度学习系统地过一遍。主要参考李宏毅和《深度学习》。在这里做一个学习记录便于后续复习参考。\n有小伙伴提到评论区无法正常使用的问题，等我后续有空再进行调整，在此之前如果大家对博客中的内容有建议/疑问欢迎通过issue交流。\n感谢阅读！\n\nML &amp; DL\nML：定义 function -&gt; 对参数（权重）优化 -&gt; 选择最佳权重\nDL：定义神经网络 -&gt; 对参数（权重）优化 -&gt; 选择最佳权重\n\n我们可以把一个 Logistic Regression 理解为一个神经元，用不同方法连接这些神经元，就得到了不同的神经网络。\n\n比如，分成多列、完全连接，就得到了全连接神经网络。\n\n如上图，上图是一个确定的全连接神经网络，我们可以把它看作是一个 function ，它的输入是一个向量，输出也是一个向量。\n\n如果改变这个 function 里的神经元 weights 和 bias，就会得到另一个 function。换言之，这个结构定义了一个函数集合。DL的训练过程其实就是通过梯度下降等方法找到让 function 表现最好的W和b们。\n在做具体预测的时候，我们会用矩阵操作进行计算，每一个 weights 用一个矩阵表示，每一个 bias 用一个向量表示，那么一个 Logistic regression 就是  ,再把传递给下一个LR。\n\nDL和 ML 相比，最大的不同是把 ML 中需要人工参与的特征提取放到神经网络中去，让神经网络来寻找能表示训练数据的特征向量。DL 可以解决人类不能解释的问题（如语音识别等）， ML 适合做人类能比较好地描述特征的任务。\n反向传播在通过梯度下降优化  和  时，神经网络和 Linear Regression 没有区别，都是先求偏导，更新 W 和 b。难点在于神经网络的函数更加复杂，需要通过链式求导来计算梯度。\n下面介绍一个简单的例子。\n\n将某一个训练数据传入神经网络，得到预测的,定义这个预测值和真实值的差距为，那么整个数据集的损失函数为：\n\n对于其中某一个, 有：\n\n只要计算某一个训练数据对的偏导，再对所有训练数据进行sum，就能得到整个数据集对该的偏导，下面计算单个数据的偏导。\n对于每一个神经元：\n\n根据链式法则，C对于这个的偏导是：\n\n其中， 比较好计算，即对应的  值（Forward pass）。\n\n 比较复杂（Backward pass）。假设通过激活函数后的值为, 则这个值又可以写为：\n\n其中，是对激活函数求导。\n\n和之前的\n\n计算异曲同工，\n\n其中，和的计算同样是对应的  值。则\n\\frac{\\partial C}{\\partial z} = \\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}=sigmoid'(z)[w_3\\frac{\\partial C}{\\partial z'}+w_4\\frac{\\partial C}{\\partial z''}]接下来分两种情况讨论，\n\n如果和是最后一层，则计算非常简单：\n\n\n\n如果不是最后一层，需要逐层从后往前回溯\n\n总结BP算法：先正向传播，计算, 再反向传播，计算，则：\n\n理论上DL能拟合任意曲线\n如上图的红色折线，可以用多条蓝色折线组合出来。当折线足够多的时候，可以近似地拟合曲线。也就是说，只要就足够多的蓝色曲线，就可以拟合任意的连续曲线。\n\n\n这个蓝色曲线，可以用sigmoid来理解\n\n这里的 Sigmoid ，其实还有很多别的选择，比如 ReLU， 两个ReLU叠起来，可以变成一个 Hard Sigmoid。这些用来拟合曲线的简单曲线，就是激活函数。\n最简单的DL其实就是做了这样的事情，用激活函数去拟合任意函数。\n","slug":"20210829","date":"2021-08-29T06:56:50.000Z","categories_index":"Note","tags_index":"ML","author_index":"Fairy"},{"id":"c570dc3eddcfb883ac8ac92a0eb2658a","title":"神经架构搜索简介","content":"今天水一天，放一章之前入门 NAS 时做的整理~\n\nNAS的三个研究方向\n\n搜索空间\n\n定义网络的表达形式\n结合特定任务的先验知识，可以减小搜索空间的大小，但同时也引入了人类的偏见，可能会限制人类找到更好的未知结构 \n\n\n搜索策略\n\n构建模型\n一方面要提高收敛速度，另一方面应该避免过早收敛到次优结构\n\n\n性能评价策略\n\n直接在数据集上跑模型非常耗时，需要更好的评价策略\n判断模型，反馈给搜索策略，搜索策略根据反馈重新生成模型\n\n\n\n搜索空间\n链式结构（逐层）\n\n难点：参数化\n最大层数（可能无限）\n每层的操作（池化、卷积、……）\n每层操作对应的参数\n\n\n\n\n多分支网络（逐层）\n\n每一层的输入是之前层的组合\n特例：\n链式结构\nResidual Networks（summed）\nDenseNets（concatenated）\n\n\n\n\n逐块:普通块和归约块、堆叠\n\n优势：\n搜索空间减少\n更好迁移（？）\n通过重读构建块来创建体系已经被证明的是很好的设计原则\n\n\n同时，出现了新的设计选择:\n微结构和整体结构都要最优\n微结构（可以借鉴现有的经典网络）\n\n\n\n\n\n搜索策略\n基于梯度的算法\n贝叶斯优化\n进化算法\n随机搜索（RS）\n强化学习（RL）\n遗传算法\n\n性能评估\n低保真度\n曲线外推\nNetwork Morphism  根据以前训练过的其他架构的权重来初始化新架构的权重\nOne-Shot Architecture Search  将所有体系结构视为一个超图的不同子图\n\n","slug":"20210826","date":"2021-08-25T08:48:50.000Z","categories_index":"Review","tags_index":"NAS","author_index":"Fairy"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"My first blog!本拖延癌晚期患者终于搭建了自己的博客框架 ~尽量日更~\n","slug":"20210825","date":"2021-08-25T08:48:50.000Z","categories_index":"Diary","tags_index":"Thinking","author_index":"Fairy"}]